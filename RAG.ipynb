{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a95c0-2abc-4364-b76c-96ffabe0d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient,model\n",
    "\n",
    "client = MilvusClient(uri=\"http://localhost:19530\", token=\"root:Milvus\")\n",
    "\n",
    "\n",
    "if client.has_collection(collection_name=\"demo_collection\"):\n",
    "    client.drop_collection(collection_name=\"demo_collection\")\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=768,  # The vectors we will use in this demo has 768 dimensions\n",
    ")\n",
    "embedding_fn = model.DefaultEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4af2ed-12e3-426b-8043-af34384b5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_file(file_path, chunk_size):\n",
    "   \n",
    "    chunks = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_chunk = []\n",
    "        for line in file:\n",
    "            if line.strip():  \n",
    "                current_chunk.append(line.strip())\n",
    "                if len(current_chunk) == chunk_size:\n",
    "                    chunks.append(\"\\n\".join(current_chunk))\n",
    "                    current_chunk = []\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845aeccb-8e87-4a81-a599-60e25b55199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"input.txt\"  \n",
    "chunk_size = 1  \n",
    "chunks = chunk_text_file(\"dataset.txt\", chunk_size)\n",
    "\n",
    "#print(chunks)\n",
    "docs=chunks\n",
    "\n",
    "vectors = embedding_fn.encode_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac7e6d-5549-49bf-a087-0f5deb4d2505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": i, \"vector\": vectors[i], \"text\": docs[i]}\n",
    "    for i in range(len(vectors))\n",
    "]\n",
    "\n",
    "res = client.insert(collection_name=\"demo_collection\", data=data)\n",
    "query_vectors = embedding_fn.encode_queries([\"What is cancer in a single sentence?\"])\n",
    "threshold = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026487ee-32a6-4c8f-a5d8-8504093a3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform search in Milvus\n",
    "result = client.search(\n",
    "    collection_name=\"demo_collection\",  # target collection\n",
    "    data=query_vectors,  # query vectors\n",
    "    limit=100,  # fetch more results to apply threshold filtering\n",
    "    output_fields=[\"text\"],  # specifies fields to be returned\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cffd3e-f851-43e6-8ee8-f3a853ddda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities and apply threshold filtering\n",
    "names = []\n",
    "for inner_list in result:\n",
    "    for item in inner_list:\n",
    "        similarity_score = item.get(\"score\", 0)  # Get the similarity score\n",
    "        if similarity_score > threshold:  # Apply threshold\n",
    "            names.append(item.get(\"entity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c1cb9-efae-4222-bb2a-f52c1fbde20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve top 5 chunks based on the filtered results\n",
    "sentences = []\n",
    "for item in names[:5]:  # Take the top 5 filtered chunks\n",
    "    if 'text' in item:\n",
    "        sentences.append(item['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e263ee-b9df-4cbf-8197-b6cf3563d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "def query_local_gpt(prompt, model_name=\"gpt2\"):\n",
    "  \n",
    "    try:\n",
    "        # Load the tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate a response\n",
    "        outputs = model.generate(inputs, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode and return the response\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c0066-cdcc-408b-a9dc-88ae926ef0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "augmented_input = \" \".join(sentences) + \" [QUERY] \" + \"What is cancer in a single sentence?\"\n",
    "response = query_local_gpt(augmented_input, model_name=\"gpt2\")  # You can replace \"gpt2\" with other models like \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "print(\"Local GPT Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
